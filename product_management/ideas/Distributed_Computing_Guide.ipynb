{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Here are the two detailed \"how-to\" guides you requested, outlining the setup for distributed chess engine computing and the creation of new machine learning-based AI engines. These guides consolidate information from our previous discussions and your provided project files, aiming to be actionable for implementation with the help of Gemini AI in VS Code Copilot.\n",
        "\n",
        "-----\n",
        "\n",
        "# 1\\. Distributed Chess Engine Computing How-To Guide\n",
        "\n",
        "This guide details how to set up a distributed computing network for your Viper Chess Engine, leveraging your existing Windows PCs and extending to a low-cost cloud environment like AWS. The goal is to efficiently run thousands or millions of game simulations and gather data for AI experimentation.\n",
        "\n",
        "## Core Principles for Distributed Chess Engine Testing\n",
        "\n",
        "Before diving into implementation, understanding these principles is key:\n",
        "\n",
        "  * **Decoupling**: Separate your application into distinct, independent services:\n",
        "      * **Game Runner/Worker**: An instance of your chess engine that plays games and generates data. This should be a lightweight, single-purpose unit.\n",
        "      * **Metrics Storage**: A centralized database to store all game results, move-by-move data, and configuration details. This is the single source of truth for your analytics.\n",
        "      * **Orchestration/Job Queue**: A system to manage and distribute game-playing tasks to available workers.\n",
        "      * **Dashboard**: A visualization layer to monitor progress and analyze collected data.\n",
        "  * **Stateless Workers**: Each game runner/worker should not retain state between game runs. It receives a task, executes it, reports results, and then is ready for a new, independent task. This simplifies scaling and fault tolerance.\n",
        "  * **Centralized Data**: All output data (PGNs, detailed move metrics, game results, configurations) must be sent to a single, shared database. This is crucial for aggregated analysis and A/B testing.\n",
        "  * **Scalability**: The architecture should allow you to easily add more workers (local PCs or cloud instances) to increase computational throughput without significant re-configuration.\n",
        "\n",
        "## Step 1: Containerization with Docker\n",
        "\n",
        "Docker allows you to package your application and its dependencies into a standardized unit (a container), ensuring it runs consistently across different environments (your various Windows PCs, Linux micro-PCs, or cloud servers).\n",
        "\n",
        "### 1.1 Install Docker\n",
        "\n",
        "  * **For Windows 10/11 Desktops/Laptops**: Install [Docker Desktop for Windows](https://docs.docker.com/desktop/install/windows-install/). Ensure WSL 2 (Windows Subsystem for Linux 2) is enabled as Docker Desktop uses it for its backend.\n",
        "  * **For Linux (Orange Pi, Raspberry Pi)**: Install [Docker Engine](https://docs.docker.com/engine/install/) directly.\n",
        "\n",
        "### 1.2 Create a `Dockerfile`\n",
        "\n",
        "In the root of your project, create a file named `Dockerfile`:\n",
        "\n",
        "```dockerfile\n",
        "# Dockerfile\n",
        "# Use a slim Python base image for smaller container size\n",
        "FROM python:3.10-slim-buster\n",
        "\n",
        "# Set the working directory in the container\n",
        "WORKDIR /app\n",
        "\n",
        "# Copy the requirements.txt file and install dependencies\n",
        "# This step is cached if requirements.txt doesn't change, speeding up builds\n",
        "COPY requirements.txt .\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "# Copy the entire project directory into the container\n",
        "COPY . .\n",
        "\n",
        "# Set environment variables if needed (e.g., for database connection, Stockfish path)\n",
        "# Stockfish path will be relative to /app/engine_utilities/external_engines/stockfish/\n",
        "# DB_HOST, DB_PORT, DB_USER, DB_PASSWORD, DB_NAME should be set dynamically or in Kubernetes/ECS\n",
        "ENV VIPER_DB_HOST=\"localhost\" \\\n",
        "    VIPER_DB_PORT=\"5432\" \\\n",
        "    VIPER_DB_USER=\"chessuser\" \\\n",
        "    VIPER_DB_PASSWORD=\"securepassword\" \\\n",
        "    VIPER_DB_NAME=\"chess_metrics_db\" \\\n",
        "    STOCKFISH_EXEC_PATH=\"/app/engine_utilities/external_engines/stockfish/stockfish-windows-x86-64-avx2.exe\"\n",
        "\n",
        "# Expose any ports if your application has a web interface (like the dashboard)\n",
        "# EXPOSE 8050 # For the Dash dashboard\n",
        "\n",
        "# Command to run your chess_game.py when the container starts\n",
        "# The arguments passed will be available in sys.argv in chess_game.py\n",
        "CMD [\"python\", \"chess_game.py\"]\n",
        "```\n",
        "\n",
        "### 1.3 Create `requirements.txt`\n",
        "\n",
        "Ensure your `requirements.txt` includes all Python packages used:\n",
        "\n",
        "```\n",
        "pygame\n",
        "python-chess\n",
        "numpy\n",
        "PyYAML\n",
        "pandas\n",
        "psutil\n",
        "dash\n",
        "plotly\n",
        "sqlite3 # Although moving to PostgreSQL/MySQL, some modules might still rely on it internally or for initial setup\n",
        "# For PostgreSQL:\n",
        "psycopg2-binary\n",
        "SQLAlchemy\n",
        "```\n",
        "\n",
        "### 1.4 Build the Docker Image\n",
        "\n",
        "Navigate to your project root in the terminal and run:\n",
        "\n",
        "```bash\n",
        "docker build -t viper-chess-engine .\n",
        "```\n",
        "\n",
        "This will create a Docker image named `viper-chess-engine` which bundles your code and dependencies.\n",
        "\n",
        "## Step 2: Centralized Database\n",
        "\n",
        "SQLite is file-based and unsuitable for multiple concurrent writes from distributed workers, especially over a network. You need a robust, centralized database. PostgreSQL is an excellent open-source choice.\n",
        "\n",
        "### 2.1 Choose a Database Solution\n",
        "\n",
        "  * **Local Network**: Deploy **PostgreSQL** on one of your more powerful Windows desktops (e.g., the i9-11900k machine). You can run it directly or within Docker.\n",
        "      * **Direct Installation (Windows)**: Download and install PostgreSQL from [postgresql.org](https://www.postgresql.org/download/windows/).\n",
        "      * **Dockerized PostgreSQL**:\n",
        "        ```bash\n",
        "        docker run --name some-postgres -e POSTGRES_PASSWORD=securepassword -p 5432:5432 -d postgres\n",
        "        ```\n",
        "        This runs PostgreSQL in a container on a specified machine. Other machines can connect to `[IP_OF_POSTGRES_HOST]:5432`.\n",
        "  * **Cloud (AWS)**: Use **Amazon RDS for PostgreSQL**. This is a managed service, meaning AWS handles setup, backups, patching, and scaling. It's not free tier eligible for continuous use but can be cost-effective for burst testing.\n",
        "\n",
        "### 2.2 Update `MetricsStore` for PostgreSQL\n",
        "\n",
        "Modify `metrics_store.py` to connect to PostgreSQL (or another SQL database) using SQLAlchemy."
      ],
      "metadata": {
        "id": "3Z0j5EkxMOIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# metrics_store.py (excerpt - modifications to existing file)\n",
        "import os\n",
        "import sqlite3 # Keep for potential local dev fallback or initial setup if needed\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "import pandas as pd\n",
        "import yaml\n",
        "import glob\n",
        "import threading\n",
        "import chess.pgn\n",
        "import io\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "# --- New Imports for SQLAlchemy and PostgreSQL ---\n",
        "from sqlalchemy import create_engine, text, inspect\n",
        "from sqlalchemy.exc import OperationalError, ProgrammingError\n",
        "from sqlalchemy.schema import Table, MetaData, Column\n",
        "from sqlalchemy import String, Integer, Float, Text, DateTime\n",
        "# --------------------------------------------------\n",
        "\n",
        "class MetricsStore:\n",
        "    def __init__(self, db_url: Optional[str] = None):\n",
        "        # Determine if using SQLite or a relational DB based on db_url\n",
        "        if db_url is None:\n",
        "            # Fallback to SQLite if no DB URL provided (e.g., for local dev/testing without external DB)\n",
        "            self.db_path = \"metrics/chess_metrics.db\"\n",
        "            os.makedirs(os.path.dirname(self.db_path), exist_ok=True)\n",
        "            self.engine = create_engine(f\"sqlite:///{self.db_path}\", connect_args={'timeout': 30})\n",
        "            self.is_sqlite = True\n",
        "        else:\n",
        "            self.db_url = db_url\n",
        "            self.engine = create_engine(self.db_url, pool_pre_ping=True, pool_recycle=3600)\n",
        "            self.is_sqlite = False\n",
        "\n",
        "        self.lock = threading.RLock()\n",
        "        self.local = threading.local()\n",
        "        self.metadata = MetaData() # For SQLAlchemy reflection\n",
        "\n",
        "        self._initialize_database()\n",
        "\n",
        "        self.collection_active = False\n",
        "        self.collection_thread = None\n",
        "\n",
        "    def _get_connection(self):\n",
        "        # Use SQLAlchemy engine for connection management\n",
        "        if not hasattr(self.local, 'connection') or self.local.connection is None:\n",
        "            try:\n",
        "                self.local.connection = self.engine.connect()\n",
        "                # For SQLite, ensure WAL and normal sync are set for better performance\n",
        "                if self.is_sqlite:\n",
        "                    self.local.connection.execute(text('PRAGMA journal_mode=WAL'))\n",
        "                    self.local.connection.execute(text('PRAGMA synchronous=NORMAL'))\n",
        "                self.local.connection.begin() # Start a transaction immediately\n",
        "            except Exception as e:\n",
        "                print(f\"Error getting database connection: {e}\")\n",
        "                self.local.connection = None # Ensure it's None on failure\n",
        "                raise\n",
        "        return self.local.connection\n",
        "\n",
        "    def _initialize_database(self):\n",
        "        # Use SQLAlchemy's metadata and Table objects for schema definition\n",
        "        # This allows for a more database-agnostic schema definition and migration\n",
        "        with self._get_connection() as conn:\n",
        "            # Define tables\n",
        "            log_entries_table = Table(\n",
        "                'log_entries', self.metadata,\n",
        "                Column('id', Integer, primary_key=True, autoincrement=True),\n",
        "                Column('timestamp', String),\n",
        "                Column('function_name', String),\n",
        "                Column('log_file', String),\n",
        "                Column('message', Text),\n",
        "                Column('value', Float),\n",
        "                Column('label', String),\n",
        "                Column('side', String),\n",
        "                Column('fen', Text),\n",
        "                Column('raw_text', Text, unique=True),\n",
        "                Column('created_at', DateTime, default=datetime.now())\n",
        "            )\n",
        "            game_results_table = Table(\n",
        "                'game_results', self.metadata,\n",
        "                Column('id', Integer, primary_key=True, autoincrement=True),\n",
        "                Column('game_id', String, unique=True),\n",
        "                Column('timestamp', String),\n",
        "                Column('winner', String),\n",
        "                Column('game_pgn', Text),\n",
        "                Column('white_player', String),\n",
        "                Column('black_player', String),\n",
        "                Column('game_length', Integer),\n",
        "                Column('created_at', DateTime, default=datetime.now()),\n",
        "                Column('white_ai_type', String),\n",
        "                Column('black_ai_type', String),\n",
        "                Column('white_depth', Integer),\n",
        "                Column('black_depth', Integer)\n",
        "            )\n",
        "            config_settings_table = Table(\n",
        "                'config_settings', self.metadata,\n",
        "                Column('id', Integer, primary_key=True, autoincrement=True),\n",
        "                Column('config_id', String, unique=True),\n",
        "                Column('timestamp', String),\n",
        "                Column('game_id', String),\n",
        "                Column('config_data', Text),\n",
        "                Column('white_engine', String),\n",
        "                Column('black_engine', String),\n",
        "                Column('white_depth', Integer),\n",
        "                Column('black_depth', Integer),\n",
        "                Column('created_at', DateTime, default=datetime.now()),\n",
        "                Column('white_ai_type', String),\n",
        "                Column('black_ai_type', String)\n",
        "            )\n",
        "            metrics_table = Table(\n",
        "                'metrics', self.metadata,\n",
        "                Column('id', Integer, primary_key=True, autoincrement=True),\n",
        "                Column('metric_name', String),\n",
        "                Column('metric_value', Float),\n",
        "                Column('side', String),\n",
        "                Column('function_name', String),\n",
        "                Column('timestamp', String),\n",
        "                Column('game_id', String),\n",
        "                Column('config_id', String),\n",
        "                Column('created_at', DateTime, default=datetime.now()),\n",
        "                # Adding composite unique constraint via SQL, not directly in SQLAlchemy for simplicity here\n",
        "            )\n",
        "            move_metrics_table = Table(\n",
        "                'move_metrics', self.metadata,\n",
        "                Column('id', Integer, primary_key=True, autoincrement=True),\n",
        "                Column('game_id', String),\n",
        "                Column('move_number', Integer),\n",
        "                Column('player_color', String),\n",
        "                Column('move_uci', String),\n",
        "                Column('fen_before', Text),\n",
        "                Column('evaluation', Float),\n",
        "                Column('ai_type', String),\n",
        "                Column('depth', Integer),\n",
        "                Column('nodes_searched', Integer),\n",
        "                Column('time_taken', Float),\n",
        "                Column('pv_line', Text),\n",
        "                Column('created_at', DateTime, default=datetime.now()),\n",
        "                # Adding composite unique constraint via SQL, not directly in SQLAlchemy for simplicity here\n",
        "            )\n",
        "\n",
        "            self.metadata.create_all(self.engine) # Creates tables if they don't exist\n",
        "\n",
        "            # Ensure unique constraints and indices are added if not already there\n",
        "            # (e.g., for `metrics` and `move_metrics` composite unique constraints)\n",
        "            # For simplicity, we assume the initial CREATE TABLE statements handle these,\n",
        "            # or they are added as separate DDL commands outside this init, as ALTER TABLE\n",
        "            # for composite unique constraints is complex and DB-specific.\n",
        "\n",
        "            conn.commit() # Commit changes after creating tables\n",
        "\n",
        "    def _execute_with_retry(self, query, params=(), max_retries=5):\n",
        "        # Modified to use SQLAlchemy's connection execute\n",
        "        retries = 0\n",
        "        while retries < max_retries:\n",
        "            try:\n",
        "                with self._get_connection() as conn:\n",
        "                    result = conn.execute(query, params)\n",
        "                    conn.commit() # Explicit commit for each operation\n",
        "                    return result\n",
        "            except OperationalError as e:\n",
        "                if \"locked\" in str(e).lower() or \"database is locked\" in str(e): # For SQLite fallback\n",
        "                    retries += 1\n",
        "                    time.sleep(0.1 * (2 ** retries)) # Exponential backoff\n",
        "                else:\n",
        "                    raise\n",
        "            except ProgrammingError as e:\n",
        "                if \"duplicate key value violates unique constraint\" in str(e).lower() or \"UNIQUE constraint failed\" in str(e):\n",
        "                    # Ignore duplicate inserts as per OR IGNORE behavior\n",
        "                    return None\n",
        "                raise\n",
        "            except Exception as e:\n",
        "                print(f\"Unhandled error during DB operation: {e}\")\n",
        "                raise\n",
        "        raise OperationalError(\"Max retries reached for query execution.\")\n",
        "\n",
        "    def add_game_result(self, game_id: str, timestamp: str, winner: str, game_pgn: str,\n",
        "                        white_player: str, black_player: str, game_length: int,\n",
        "                        white_ai_config: dict, black_ai_config: dict):\n",
        "        # Use SQLAlchemy's text() for literal SQL or map to ORM if fully migrating\n",
        "        query = text(f\"\"\"\n",
        "            INSERT INTO game_results (game_id, timestamp, winner, game_pgn, white_player, black_player, game_length,\n",
        "                                     white_ai_type, black_ai_type, white_depth, black_depth)\n",
        "            VALUES (:game_id, :timestamp, :winner, :game_pgn, :white_player, :black_player, :game_length,\n",
        "                    :white_ai_type, :black_ai_type, :white_depth, :black_depth)\n",
        "            ON CONFLICT(game_id) DO NOTHING;\n",
        "        \"\"\")\n",
        "        params = {\n",
        "            \"game_id\": game_id,\n",
        "            \"timestamp\": timestamp,\n",
        "            \"winner\": winner,\n",
        "            \"game_pgn\": game_pgn,\n",
        "            \"white_player\": white_player,\n",
        "            \"black_player\": black_player,\n",
        "            \"game_length\": game_length,\n",
        "            \"white_ai_type\": white_ai_config.get('ai_type', 'unknown'),\n",
        "            \"black_ai_type\": black_ai_config.get('ai_type', 'unknown'),\n",
        "            \"white_depth\": white_ai_config.get('depth', 0),\n",
        "            \"black_depth\": black_ai_config.get('depth', 0)\n",
        "        }\n",
        "        self._execute_with_retry(query, params)\n",
        "\n",
        "    def add_move_metric(self, game_id: str, move_number: int, player_color: str,\n",
        "                        move_uci: str, fen_before: str, evaluation: float,\n",
        "                        ai_type: str, depth: int, nodes_searched: int,\n",
        "                        time_taken: float, pv_line: str):\n",
        "        query = text(f\"\"\"\n",
        "            INSERT INTO move_metrics (game_id, move_number, player_color, move_uci, fen_before,\n",
        "                                     evaluation, ai_type, depth, nodes_searched, time_taken, pv_line)\n",
        "            VALUES (:game_id, :move_number, :player_color, :move_uci, :fen_before,\n",
        "                    :evaluation, :ai_type, :depth, :nodes_searched, :time_taken, :pv_line)\n",
        "            ON CONFLICT(game_id, move_number, player_color) DO NOTHING;\n",
        "        \"\"\")\n",
        "        params = {\n",
        "            \"game_id\": game_id, \"move_number\": move_number, \"player_color\": player_color,\n",
        "            \"move_uci\": move_uci, \"fen_before\": fen_before, \"evaluation\": evaluation,\n",
        "            \"ai_type\": ai_type, \"depth\": depth, \"nodes_searched\": nodes_searched,\n",
        "            \"time_taken\": time_taken, \"pv_line\": pv_line\n",
        "        }\n",
        "        self._execute_with_retry(query, params)\n",
        "\n",
        "    def get_all_game_results_df(self):\n",
        "        with self._get_connection() as conn:\n",
        "            df = pd.read_sql_query(\"SELECT * FROM game_results\", conn)\n",
        "        return df\n",
        "\n",
        "    def get_distinct_move_metric_names(self):\n",
        "        # Query schema using inspect for dynamic column names\n",
        "        inspector = inspect(self.engine)\n",
        "        columns_info = inspector.get_columns('move_metrics')\n",
        "\n",
        "        plot_eligible_types = ['REAL', 'INTEGER', 'FLOAT']\n",
        "        exclude_names = ['id', 'game_id', 'move_number', 'player_color', 'move_uci', 'fen_before', 'ai_type', 'pv_line', 'created_at']\n",
        "\n",
        "        metric_names = []\n",
        "        for col in columns_info:\n",
        "            col_name = col['name']\n",
        "            col_type = str(col['type']).upper()\n",
        "\n",
        "            if col_name not in exclude_names and any(eligible_type in col_type for eligible_type in plot_eligible_types):\n",
        "                metric_names.append(col_name)\n",
        "        return sorted(metric_names)\n",
        "\n",
        "    def get_filtered_move_metrics(self, white_ai_types: Optional[list] = None, black_ai_types: Optional[list] = None, metric_name: Optional[str] = None):\n",
        "        with self._get_connection() as conn:\n",
        "            # Building WHERE clause\n",
        "            where_clauses = []\n",
        "            params = {}\n",
        "\n",
        "            if white_ai_types:\n",
        "                # Use parameterized query for IN clause\n",
        "                white_placeholders = ', '.join([f':white_ai_type_{i}' for i in range(len(white_ai_types))])\n",
        "                where_clauses.append(f\"gr.white_ai_type IN ({white_placeholders})\")\n",
        "                params.update({f'white_ai_type_{i}': val for i, val in enumerate(white_ai_types)})\n",
        "\n",
        "            if black_ai_types:\n",
        "                black_placeholders = ', '.join([f':black_ai_type_{i}' for i in range(len(black_ai_types))])\n",
        "                where_clauses.append(f\"gr.black_ai_type IN ({black_placeholders})\")\n",
        "                params.update({f'black_ai_type_{i}': val for i, val in enumerate(black_ai_types)})\n",
        "\n",
        "            select_columns = \"mm.game_id, mm.move_number, mm.player_color, mm.move_uci, mm.fen_before, mm.created_at, mm.evaluation, mm.nodes_searched, mm.time_taken, mm.depth, mm.pv_line\"\n",
        "            if metric_name:\n",
        "                if metric_name in self.get_distinct_move_metric_names():\n",
        "                    select_columns += f\", mm.{metric_name}\"\n",
        "                else:\n",
        "                    print(f\"Warning: Attempted to query invalid or non-numeric metric_name: {metric_name}\")\n",
        "                    return [] # Return empty if invalid metric_name\n",
        "\n",
        "            query_str = f\"\"\"\n",
        "            SELECT {select_columns}, gr.white_ai_type, gr.black_ai_type, gr.white_depth, gr.black_depth\n",
        "            FROM move_metrics mm\n",
        "            JOIN game_results gr ON mm.game_id = gr.game_id\n",
        "            \"\"\"\n",
        "\n",
        "            if where_clauses:\n",
        "                query_str += \" WHERE \" + \" AND \".join(where_clauses)\n",
        "\n",
        "            query_str += \" ORDER BY mm.created_at\"\n",
        "\n",
        "            df = pd.read_sql_query(text(query_str), conn, params=params)\n",
        "        return df.to_dict(orient='records') # Return as list of dicts for Dash\n",
        "\n",
        "    def close(self):\n",
        "        if hasattr(self.local, 'connection') and self.local.connection:\n",
        "            try:\n",
        "                self.local.connection.commit() # Commit any pending transactions\n",
        "            except Exception as e:\n",
        "                print(f\"Error during final commit: {e}\")\n",
        "            self.local.connection.close()\n",
        "            self.local.connection = None\n",
        "        if self.collection_active:\n",
        "            self.stop_collection()\n",
        "        # Dispose of the engine if it's not SQLite to close pooled connections\n",
        "        if not self.is_sqlite and self.engine:\n",
        "            self.engine.dispose()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "Q-jtQw7vMOIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Changes in `metrics_store.py`:**\n",
        "\n",
        "  * **`__init__`**: Now accepts an optional `db_url`. If `None`, it defaults to SQLite. Otherwise, it uses `sqlalchemy.create_engine` to connect to the specified database (e.g., PostgreSQL).\n",
        "  * **`_initialize_database`**: Uses `sqlalchemy.schema.Table` and `MetaData.create_all` to define and create tables in a database-agnostic way. This automatically handles table creation and column additions more gracefully than raw SQL `ALTER TABLE` for cross-DB compatibility.\n",
        "  * **`_get_connection`**: Manages connections using SQLAlchemy's connection pool. It also starts a transaction, which is critical for concurrent database operations.\n",
        "  * **`_execute_with_retry`**: Adapted to use SQLAlchemy's connection execution, adding `ON CONFLICT DO NOTHING` for `INSERT` statements to replicate SQLite's `INSERT OR IGNORE` behavior and handle concurrency.\n",
        "  * **`add_game_result` / `add_move_metric`**: Updated to use SQLAlchemy's `text()` construct for SQL queries with named parameters, improving readability and security against SQL injection.\n",
        "  * **`get_distinct_move_metric_names`**: Now uses SQLAlchemy's `inspect` module to dynamically query the schema of the `move_metrics` table, ensuring it only presents valid numeric columns for plotting.\n",
        "  * **`get_filtered_move_metrics`**: Enhanced to use SQLAlchemy's `text()` and parameterized queries for dynamic filtering based on AI types and selected metrics. It explicitly joins `move_metrics` with `game_results` to access AI configuration details. Returns a list of dictionaries for easier Dash consumption.\n",
        "  * **`close`**: Ensures proper disposal of the SQLAlchemy engine if an external database is used.\n",
        "\n",
        "**Action for You:**\n",
        "\n",
        "  * You must modify `local_metrics_dashboard.py` to use the new `db_url` parameter when initializing `MetricsStore` if you want to use PostgreSQL. E.g., `metrics_store = MetricsStore(db_url=\"postgresql://chessuser:securepassword@localhost:5432/chess_metrics_db\")`. If left as `MetricsStore()`, it will continue using SQLite.\n",
        "\n",
        "## Step 3: Local Distributed Setup with Kubernetes (K3s/MicroK8s)\n",
        "\n",
        "For your Windows PCs, K3s or MicroK8s are great lightweight options for a local Kubernetes cluster. This will allow you to deploy your Dockerized chess engine workers.\n",
        "\n",
        "### 3.1 Install K3s (Recommended for simplicity on Windows)\n",
        "\n",
        "  * **Install WSL 2**: Follow Microsoft's guide to install WSL 2 on all your Windows 10/11 machines.\n",
        "  * **Install K3s on Windows (via WSL 2)**:\n",
        "    1.  On each Windows machine, open your WSL 2 Linux distribution (e.g., Ubuntu).\n",
        "    2.  Run the K3s installation script:\n",
        "        ```bash\n",
        "        curl -sfL https://get.k3s.io | sh -\n",
        "        ```\n",
        "    3.  **One machine as Master**: On one machine (e.g., your i9 desktop), set it as the master node. Get its token:\n",
        "        ```bash\n",
        "        sudo cat /var/lib/rancher/k3s/server/node-token\n",
        "        ```\n",
        "        And its IP address in WSL2: `ip a | grep eth0`\n",
        "    4.  **Other machines as Agents**: On other Windows machines, join them as agent nodes to the master. Replace `MASTER_IP` and `NODE_TOKEN`:\n",
        "        ```bash\n",
        "        curl -sfL https://get.k3s.io | K3S_URL=https://MASTER_IP:6443 K3S_TOKEN=NODE_TOKEN sh -\n",
        "        ```\n",
        "  * **Verify Cluster**: On the master node, check if nodes are ready:\n",
        "    ```bash\n",
        "    kubectl get nodes\n",
        "    ```\n",
        "\n",
        "### 3.2 Kubernetes Deployment Files\n",
        "\n",
        "You'll define Kubernetes objects (YAML files) to tell your cluster how to run your chess engine.\n",
        "\n",
        "#### `chess-engine-worker.yaml` (Kubernetes Job)\n",
        "\n",
        "This defines a `Job` that runs your chess engine container. Each `Job` will run a game (or a batch of games) and then exit.\n",
        "\n",
        "```yaml\n",
        "# chess-engine-worker.yaml\n",
        "apiVersion: batch/v1\n",
        "kind: Job\n",
        "metadata:\n",
        "  name: chess-engine-worker-{{ .Release.Time.Seconds }} # Unique name for each job instance\n",
        "spec:\n",
        "  template:\n",
        "    metadata:\n",
        "      labels:\n",
        "        app: chess-engine-worker\n",
        "    spec:\n",
        "      restartPolicy: Never # Crucial for Jobs: container completes and does not restart\n",
        "      containers:\n",
        "      - name: viper-chess-engine\n",
        "        image: viper-chess-engine:latest # Use the Docker image you built\n",
        "        imagePullPolicy: Never # Use local image if not pulling from registry\n",
        "        env:\n",
        "          - name: VIPER_DB_HOST\n",
        "            value: \"192.168.1.100\" # Replace with your PostgreSQL host IP (e.g., your i9 desktop's LAN IP)\n",
        "          - name: VIPER_DB_PORT\n",
        "            value: \"5432\"\n",
        "          - name: VIPER_DB_USER\n",
        "            value: \"chessuser\"\n",
        "          - name: VIPER_DB_PASSWORD\n",
        "            valueFrom: # Best practice: use Kubernetes Secret for passwords\n",
        "              secretKeyRef:\n",
        "                name: chess-db-secret\n",
        "                key: db-password\n",
        "          - name: VIPER_DB_NAME\n",
        "            value: \"chess_metrics_db\"\n",
        "          - name: STOCKFISH_EXEC_PATH # Path inside the container (from Dockerfile)\n",
        "            value: \"/app/engine_utilities/external_engines/stockfish/stockfish-windows-x86-64-avx2.exe\"\n",
        "          # Pass AI config dynamically (example: override for this specific job)\n",
        "          # These values would override defaults in config.yaml for this run\n",
        "          # Or, modify chess_game.py to read these from env vars directly\n",
        "          - name: WHITE_AI_TYPE\n",
        "            value: \"deepsearch\"\n",
        "          - name: BLACK_AI_TYPE\n",
        "            value: \"stockfish\"\n",
        "          - name: AI_GAME_COUNT\n",
        "            value: \"10\" # Play 10 games per job instance\n",
        "        volumeMounts:\n",
        "          - name: stockfish-volume\n",
        "            mountPath: /app/engine_utilities/external_engines/stockfish # Mount Stockfish binary\n",
        "        resources:\n",
        "          requests: # Request minimum resources\n",
        "            cpu: \"500m\" # 0.5 CPU core\n",
        "            memory: \"1Gi\" # 1 GB RAM\n",
        "          limits: # Set maximum resources\n",
        "            cpu: \"2\" # 2 CPU cores\n",
        "            memory: \"4Gi\" # 4 GB RAM\n",
        "      volumes:\n",
        "        - name: stockfish-volume\n",
        "          hostPath:\n",
        "            path: C:\\Users\\patss\\OneDrive\\Documents\\Programming\\ViperChessEngine\\v7p3r_chess_bot_simple\\engine_utilities\\external_engines\\stockfish # Host path to Stockfish folder\n",
        "            type: Directory # For Windows host paths\n",
        "\n",
        "```\n",
        "\n",
        "**Explanation for `chess-engine-worker.yaml`:**\n",
        "\n",
        "  * **`apiVersion: batch/v1`, `kind: Job`**: Defines this as a Kubernetes Job.\n",
        "  * **`restartPolicy: Never`**: Important for Jobs; the container exits after completing its task and is not restarted.\n",
        "  * **`image: viper-chess-engine:latest`**: Refers to the Docker image you built.\n",
        "  * **`imagePullPolicy: Never`**: Tells Kubernetes to not try to pull the image from a remote registry, assuming it's available locally on each node (you'll need to `docker load` or `docker push/pull` it if nodes don't share Docker daemon).\n",
        "  * **`env`**: Environment variables are passed to the container. This is how you'll pass database connection details and potentially override AI configurations for specific test runs.\n",
        "  * **`volumeMounts` / `volumes`**: This is how you'll make your Stockfish executable available *inside* the container. `hostPath` is used to mount a directory from the host machine directly into the container. **Adjust `path` to your exact Stockfish directory on your Windows hosts.**\n",
        "  * **`resources`**: Critical for resource allocation. You can specify CPU (e.g., \"500m\" for 0.5 CPU core, \"2\" for 2 CPU cores) and memory limits. This helps Kubernetes schedule jobs efficiently on your diverse hardware.\n",
        "\n",
        "#### `chess-db-secret.yaml` (Kubernetes Secret - for passwords)\n",
        "\n",
        "```yaml\n",
        "# chess-db-secret.yaml\n",
        "apiVersion: v1\n",
        "kind: Secret\n",
        "metadata:\n",
        "  name: chess-db-secret\n",
        "type: Opaque\n",
        "data:\n",
        "  db-password: YXNlY3VyZXBhc3N3b3Jk # Base64 encoded 'securepassword'\n",
        "```\n",
        "\n",
        "**To create the secret**:\n",
        "\n",
        "1.  Base64 encode your actual password: `echo -n \"your_db_password\" | base64`\n",
        "2.  Replace `YXNlY3VyZXBhc3N3b3Jk` with your encoded password.\n",
        "3.  Apply the secret: `kubectl apply -f chess-db-secret.yaml`\n",
        "\n",
        "### 3.3 Running Jobs on Kubernetes\n",
        "\n",
        "To run a batch of games:\n",
        "\n",
        "```bash\n",
        "kubectl create -f chess-engine-worker.yaml\n",
        "```\n",
        "\n",
        "You can scale up the number of parallel workers by increasing the `parallelism` field in the Job spec, or by creating multiple `Job` instances programmatically (e.g., using a Python script or a CI/CD pipeline).\n",
        "\n",
        "## Step 4: Cloud Deployment Option (AWS)\n",
        "\n",
        "For scaling beyond your home network or for continuous, high-volume testing, AWS is a powerful option.\n",
        "\n",
        "### 4.1 AWS Services Overview\n",
        "\n",
        "  * **Amazon EC2 (Elastic Compute Cloud)**: Virtual servers (VMs) where you can run your Docker containers. You'd choose instance types with powerful GPUs for ML training or high-CPU instances for search-intensive work.\n",
        "      * **Pricing**: Pay-as-you-go per second. Spot Instances offer significant discounts for fault-tolerant workloads.\n",
        "  * **Amazon ECS (Elastic Container Service) / EKS (Elastic Kubernetes Service)**: Managed container orchestration services. ECS is simpler for basic Docker deployments, while EKS provides a fully managed Kubernetes experience.\n",
        "      * **AWS Fargate**: A compute engine for ECS/EKS that lets you run containers without managing servers. You only pay for the compute resources your containers use. Ideal for short-lived, burstable jobs like game simulations.\n",
        "  * **Amazon RDS (Relational Database Service)**: Managed relational databases (PostgreSQL, MySQL, etc.). Simplifies database administration (backups, scaling, patching).\n",
        "      * **Pricing**: Instance-based, plus storage and I/O.\n",
        "  * **Amazon S3 (Simple Storage Service)**: Object storage for large files (PGNs, model checkpoints, raw log data).\n",
        "  * **AWS CloudWatch**: Monitoring and logging for all your AWS resources. Essential for observing resource usage, performance, and errors.\n",
        "  * **AWS Budgets**: Set custom cost alerts to monitor your spending and avoid surprises.\n",
        "\n",
        "### 4.2 AWS Deployment Strategy (Fargate for Workers, RDS for DB)\n",
        "\n",
        "This is a good starting point for a cost-effective and managed deployment.\n",
        "\n",
        "1.  **Set up Amazon RDS for PostgreSQL**:\n",
        "      * Go to the RDS console.\n",
        "      * Create a new PostgreSQL database instance. Choose a small instance size (e.g., `db.t3.micro` or `db.t4g.micro`) for low-cost testing.\n",
        "      * Configure public accessibility if accessing from local network for testing (disable for production for security).\n",
        "      * Note the endpoint (hostname), port, username, and password. Update your `MetricsStore`'s `db_url` and Dockerfile environment variables accordingly.\n",
        "2.  **Push Docker Image to Amazon ECR (Elastic Container Registry)**:\n",
        "      * Create a repository in ECR.\n",
        "      * Follow ECR's push commands to push your `viper-chess-engine` Docker image:\n",
        "        ```bash\n",
        "        aws ecr get-login-password --region <your-region> | docker login --username AWS --password-stdin <your-account-id>.dkr.ecr.<your-region>.amazonaws.com\n",
        "        docker tag viper-chess-engine:latest <your-account-id>.dkr.ecr.<your-region>.amazonaws.com/viper-chess-engine:latest\n",
        "        docker push <your-account-id>.dkr.ecr.<your-region>.amazonaws.com/viper-chess-engine:latest\n",
        "        ```\n",
        "3.  **Create an Amazon ECS Cluster**:\n",
        "      * In the ECS console, create a new ECS Cluster (choose Fargate as the capacity provider).\n",
        "4.  **Define an ECS Task Definition**:\n",
        "      * This specifies your Docker image, CPU/memory limits, environment variables (for DB connection), and volume mounts (if using EFS for shared data, though direct DB writes are preferred).\n",
        "      * For Stockfish executable, you'd either bundle it in the Docker image (which is what we set up) or mount it from EFS (if you need a shared, persistent file system across tasks, less common for binaries).\n",
        "      * Example task definition structure:\n",
        "        ```json\n",
        "        {\n",
        "          \"family\": \"viper-chess-engine-task\",\n",
        "          \"networkMode\": \"awsvpc\",\n",
        "          \"cpu\": \"1024\",\n",
        "          \"memory\": \"2048\",\n",
        "          \"executionRoleArn\": \"arn:aws:iam::ACCOUNT_ID:role/ecsTaskExecutionRole\",\n",
        "          \"containerDefinitions\": [\n",
        "            {\n",
        "              \"name\": \"viper-chess-engine-worker\",\n",
        "              \"image\": \"ACCOUNT_ID.dkr.ecr.REGION.amazonaws.com/viper-chess-engine:latest\",\n",
        "              \"essential\": true,\n",
        "              \"command\": [\"python\", \"chess_game.py\"],\n",
        "              \"environment\": [\n",
        "                {\"name\": \"VIPER_DB_HOST\", \"value\": \"your-rds-endpoint\"},\n",
        "                {\"name\": \"VIPER_DB_PORT\", \"value\": \"5432\"},\n",
        "                {\"name\": \"VIPER_DB_USER\", \"value\": \"chessuser\"},\n",
        "                {\"name\": \"VIPER_DB_PASSWORD\", \"valueFrom\": {\"secretManagerSecretArn\": \"arn:aws:secretsmanager:REGION:ACCOUNT_ID:secret:your-db-secret\"}}, # Use Secrets Manager\n",
        "                {\"name\": \"VIPER_DB_NAME\", \"value\": \"chess_metrics_db\"},\n",
        "                {\"name\": \"STOCKFISH_EXEC_PATH\", \"value\": \"/app/engine_utilities/external_engines/stockfish/stockfish-windows-x86-64-avx2.exe\"},\n",
        "                {\"name\": \"AI_GAME_COUNT\", \"value\": \"100\"} # Run 100 games per task\n",
        "              ],\n",
        "              \"logConfiguration\": {\n",
        "                \"logDriver\": \"awslogs\",\n",
        "                \"options\": {\n",
        "                  \"awslogs-group\": \"/ecs/viper-chess-engine\",\n",
        "                  \"awslogs-region\": \"REGION\",\n",
        "                  \"awslogs-stream-prefix\": \"ecs\"\n",
        "                }\n",
        "              }\n",
        "            }\n",
        "          ]\n",
        "        }\n",
        "        ```\n",
        "5.  **Run ECS Tasks**:\n",
        "      * In the ECS console, go to your cluster and select \"Run new Task\".\n",
        "      * Specify your task definition and the number of tasks to run (e.g., 100 tasks to run 10,000 games if each task plays 100 games).\n",
        "      * **Programmatic Execution**: For large-scale testing, you'd write a script (e.g., a Python script or AWS Lambda function) that uses the AWS SDK (Boto3) to launch many ECS tasks in parallel, possibly varying `AI_GAME_COUNT` or other AI parameters for A/B testing.\n",
        "6.  **Deploy the Dashboard (Streamlit Cloud or AWS App Runner)**:\n",
        "      * **Streamlit Cloud**: Easiest way. Connect your GitHub repo containing `local_metrics_dashboard.py` and `metrics_store.py`. Ensure your `metrics_store.py` connects to your **publicly accessible** AWS RDS endpoint. (Be mindful of security implications if exposing your DB publicly.)\n",
        "      * **AWS App Runner**: A fully managed service for deploying web applications and APIs. You provide your source code or a Docker image, and App Runner handles infrastructure. This would be a more robust and secure way to host your dashboard in AWS.\n",
        "7.  **Monitoring and Cost Management (CloudWatch, Budgets)**:\n",
        "      * **CloudWatch**: Monitor CPU, memory, and network usage of your ECS tasks. Set up alarms for high resource utilization or task failures. Stream container logs to CloudWatch Logs for debugging.\n",
        "      * **AWS Budgets**: Set up monthly budgets for your EC2/Fargate and RDS costs. Configure alerts (email/SNS) when your actual or forecasted spend approaches your budget limit. This is crucial for controlling costs in the cloud.\n",
        "\n",
        "### 4.3 Key Considerations for Your Hardware\n",
        "\n",
        "  * **Mixed Hardware**: Your diverse set of Windows PCs (i9, i5, older desktop, laptop, mini-PCs) can all serve as Docker hosts and Kubernetes nodes. The `requests` and `limits` in Kubernetes YAMLs are important to ensure tasks are scheduled on nodes with sufficient resources.\n",
        "  * **GPU Utilization**: For future ML integration, direct GPU access in Kubernetes (especially on Windows) is complex.\n",
        "      * **Local Kubernetes**: Requires specific drivers and GPU-aware Docker runtimes (e.g., NVIDIA Container Toolkit) on each node. K3s has some experimental support for GPU passthrough.\n",
        "      * **Cloud (AWS)**: EC2 instances offer GPU instances (e.g., `p3`, `g4dn`, `g5` families) that come with NVIDIA drivers pre-installed. Fargate generally does not offer GPU instances, so you'd use EC2 instances managed by ECS/EKS for GPU workloads.\n",
        "  * **Storage**: For local Kubernetes, you might need a Network File System (NFS) or SMB share if your workers need shared file access (e.g., for large datasets for ML training). For metrics, stick to the central database.\n",
        "  * **Network (LAN)**: Ensure reliable network connectivity between all your local PCs and the PostgreSQL host. Hardwiring (Gigabit Ethernet) is always preferred over Wi-Fi for stability and performance.\n",
        "\n",
        "This detailed guide provides a roadmap for your distributed computing setup.\n",
        "\n",
        "-----\n",
        "\n",
        "# 2\\. Machine Learning Engine Creation and Enhancements How-To Guide\n",
        "\n",
        "This guide outlines how to build four additional AI types for your Viper Chess Engine, leveraging machine learning techniques and integrating them with your existing evaluation logic and the overall `chess_game.py` framework. This will allow your AI to learn from data and improve its play, and be compatible with external chess GUIs.\n",
        "\n",
        "## Overview of New AI Types\n",
        "\n",
        "We will define how to implement the following AI types:\n",
        "\n",
        "1.  **Viper NN AI Engine (Supervised Learning)**: Learns moves from existing human-played PGN data.\n",
        "2.  **Viper Genetic AI Engine**: Evolves evaluation parameters using genetic algorithms, driven by performance against other AIs.\n",
        "3.  **Viper Reinforcement AI Engine**: Learns through self-play by receiving rewards/penalties from game outcomes and evaluation scores.\n",
        "4.  **Viper Hybrid NN-Search Engine**: Integrates a neural network directly into your existing search algorithms (e.g., for leaf node evaluation or move ordering).\n",
        "\n",
        "## Common Integration Points for New AI Types\n",
        "\n",
        "All new AI types will need to conform to an interface similar to your existing `EvaluationEngine` and `StockfishHandler` to be plug-and-play within `chess_game.py`.\n",
        "\n",
        "### 2.0 General Structure for a New AI Engine\n",
        "\n",
        "Each new AI engine type will primarily exist as a new Python class, similar to `EvaluationEngine` and `StockfishHandler`.\n",
        "\n",
        "It will need:\n",
        "\n",
        "  * **`__init__(self, board: chess.Board, player: chess.Color, ai_config: dict)`**: Initializes the engine with the current board, player color, and specific AI configuration (from `config.yaml`).\n",
        "  * **`search(self, board: chess.Board, player: chess.Color, ai_config: dict, stop_callback=None)`**: This is the primary method called by `chess_game.py` to get a move. It should return a `chess.Move` object.\n",
        "  * **`evaluate_position_from_perspective(self, board: chess.Board, player: chess.Color)`**: Returns a numerical evaluation of the board from the given player's perspective. This is crucial for the dashboard and for internal reward/penalty functions.\n",
        "  * **`reset(self, board: chess.Board)`**: Resets the AI's internal state for a new game.\n",
        "  * **Configuration in `config.yaml`**: Add a new `ai_type` and `engine` entry, along with any specific parameters for that AI (e.g., `model_path`, `mutation_rate`).\n",
        "\n",
        "## 2.1 Viper NN AI Engine (Supervised Learning)\n",
        "\n",
        "This AI learns to play by mimicking moves from human-played games (your PGN data).\n",
        "\n",
        "### 2.1.1 Core Components (Based on `v7p3r_chess_ai_old_2025-05-31/train.py` and `chess_core.py`)\n",
        "\n",
        "  * **`chess_core.py` (or similar)**:\n",
        "      * **`ChessDataset(Dataset)`**: A PyTorch `Dataset` that reads your PGN files. It converts chess board positions into a numerical tensor representation (e.g., a 12x8x8 tensor representing pieces on squares) and associates them with the UCI string of the move played by \"you\" (the specified `username` in the PGN header).\n",
        "      * **`board_to_tensor(board)`**: A function within `ChessDataset` (or as a standalone utility) that transforms a `chess.Board` object into the numerical input tensor for the neural network.\n",
        "      * **`ChessAI(nn.Module)`**: Your neural network architecture. The provided example uses convolutional layers for spatial patterns and fully connected layers for move prediction. It outputs logits for each possible move."
      ],
      "metadata": {
        "id": "STFUEV-mMOIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simplified ChessAI (from provided files)\n",
        "        import torch.nn as nn\n",
        "        import torch.nn.functional as F\n",
        "\n",
        "        class ChessAI(nn.Module):\n",
        "            def __init__(self, num_classes): # num_classes is the size of your move vocabulary\n",
        "                super().__init__()\n",
        "                self.conv1 = nn.Conv2d(12, 64, kernel_size=3, padding=1)\n",
        "                self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "                self.conv3 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
        "                self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
        "                self.fc2 = nn.Linear(512, 256)\n",
        "                self.fc3 = nn.Linear(256, num_classes) # Output logits for each move\n",
        "                # Optionally, a value head for evaluation (like Stockfish NNUE)\n",
        "                self.value_fc1 = nn.Linear(64 * 8 * 8, 256)\n",
        "                self.value_fc2 = nn.Linear(256, 1)\n",
        "\n",
        "            def forward(self, x):\n",
        "                x = F.relu(self.conv1(x))\n",
        "                x = F.relu(self.conv2(x))\n",
        "                x = F.relu(self.conv3(x))\n",
        "                x = x.view(x.size(0), -1) # Flatten for FC layers\n",
        "\n",
        "                # Policy head (move prediction)\n",
        "                policy_output = F.relu(self.fc1(x))\n",
        "                policy_output = F.relu(self.fc2(policy_output))\n",
        "                policy_output = self.fc3(policy_output)\n",
        "\n",
        "                # Value head (position evaluation)\n",
        "                value_output = F.relu(self.value_fc1(x))\n",
        "                value_output = self.value_fc2(value_output)\n",
        "\n",
        "                return policy_output, value_output # Return both for supervised learning"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "dwr9p8pLMOIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **`train.py` (or similar)**:\n",
        "      * **`MoveEncoder`**: Maps unique UCI move strings to integer indices and vice-versa. This is essential for the NN's output layer and for converting predicted move indices back to `chess.Move` objects.\n",
        "      * **Training Loop**: Loads your PGNs, converts them into a dataset using `ChessDataset`, and trains the `ChessAI` model using a supervised learning approach (e.g., `nn.CrossEntropyLoss` for move prediction and `nn.MSELoss` for value prediction).\n",
        "          * **Objective**: Minimize the difference between predicted moves/evaluations and the actual moves/evaluations from your PGNs.\n",
        "      * **Saving/Loading Model**: Saves the trained `state_dict` of the `ChessAI` model (e.g., as `v7p3r_chess_ai_model.pth`).\n",
        "\n",
        "### 2.1.2 Integration into `chess_game.py`\n",
        "\n",
        "1.  **Load Model and Encoder**: In your `ChessGame.__init__()`, load the trained NN model and `MoveEncoder`."
      ],
      "metadata": {
        "id": "nbm8BEF4MOIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In ChessGame.__init__()\n",
        "    # ... other initializations ...\n",
        "    self.move_encoder = MoveEncoder() # Assuming MoveEncoder class is available\n",
        "\n",
        "    # Load move vocabulary (mapping of moves to indices)\n",
        "    with open(\"move_vocab.pkl\", \"rb\") as f: # You need to create this file during training\n",
        "        self.move_to_index = pickle.load(f)\n",
        "        self.index_to_move = {v: k for k, v in self.move_to_index.items()} # Reverse map\n",
        "\n",
        "    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.nn_model = ChessAI(num_classes=len(self.move_to_index)).to(self.device)\n",
        "    self.nn_model.load_state_dict(\n",
        "        torch.load(\"v7p3r_chess_ai_model.pth\", map_location=self.device, weights_only=False)\n",
        "    )\n",
        "    self.nn_model.eval() # Set model to evaluation mode\n",
        "    # ..."
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "_yflvFfyMOII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  **New AI Type in `process_ai_move`**: Add a new `elif` block in `process_ai_move` to handle `ai_type == 'viper_nn'`."
      ],
      "metadata": {
        "id": "oPkX7xqXMOII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In EvaluationEngine.search (or where AI decision is made in chess_game.py)\n",
        "    elif self.ai_type == 'viper_nn_ai_engine': # New AI type name\n",
        "        with torch.no_grad(): # No gradients needed for inference\n",
        "            # Convert board to tensor\n",
        "            board_tensor = board_to_tensor(self.board).unsqueeze(0).to(self.device)\n",
        "            policy_logits, value_eval = self.nn_model(board_tensor)\n",
        "\n",
        "            # For move selection, choose the move with the highest logit\n",
        "            # You might want to add some exploration (epsilon-greedy) for training\n",
        "            probabilities = F.softmax(policy_logits, dim=1).squeeze(0).cpu().numpy()\n",
        "\n",
        "            # Filter for legal moves only\n",
        "            legal_moves = list(self.board.legal_moves)\n",
        "            legal_move_indices = [self.move_to_index[m.uci()] for m in legal_moves if m.uci() in self.move_to_index]\n",
        "\n",
        "            # Create a probability distribution over legal moves\n",
        "            legal_probabilities = np.zeros_like(probabilities)\n",
        "            if legal_move_indices:\n",
        "                legal_probabilities[legal_move_indices] = probabilities[legal_move_indices]\n",
        "                legal_probabilities /= legal_probabilities.sum() # Normalize\n",
        "\n",
        "            if len(legal_moves) > 0 and legal_probabilities.sum() > 0:\n",
        "                # Choose move based on probabilities\n",
        "                chosen_idx = np.random.choice(len(self.index_to_move), p=legal_probabilities)\n",
        "                chosen_move_uci = self.index_to_move[chosen_idx]\n",
        "                ai_move = chess.Move.from_uci(chosen_move_uci)\n",
        "                if not self.board.is_legal(ai_move): # Fallback if chosen move is somehow illegal\n",
        "                    ai_move = random.choice(legal_moves)\n",
        "            else: # If no legal moves or no valid moves from NN\n",
        "                ai_move = random.choice(list(self.board.legal_moves)) # Fallback to random\n",
        "\n",
        "            self.current_eval = value_eval.item() # Update current_eval from NN's value head"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "-NQkxEyHMOII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Viper Genetic AI Engine\n",
        "\n",
        "This AI learns by evolving its evaluation parameters through genetic algorithms, with fitness determined by playing games using your existing evaluation logic.\n",
        "\n",
        "### 2.2.1 Core Components (Based on `v7p3r_chess_ai_old_2025-05-31/genetic_algorithm.py` and `config.yaml` rulesets)\n",
        "\n",
        "  * **`GeneticAlgorithm` Class**: From `v7p3r_chess_ai_old_2025-05-31/genetic_algorithm.py`.\n",
        "      * **`initialize_population(model_template)`**: Creates an initial population of AI \"models\" (which are essentially sets of `config.yaml` evaluation parameters). Each \"model\" is a copy of your `EvaluationEngine` but with randomized `genetic_params` (e.g., `material_weight`, `center_control_bonus`, `king_safety_bonus`, etc.).\n",
        "      * **`evaluate_fitness(model, games)`**: This is where your existing `EvaluationEngine` shines. Each `model` (set of parameters) is used to play games against a benchmark (e.g., your current best Viper, Stockfish, or even other genetic AIs). The fitness score for a `model` is derived from its game results (win/loss/draw, average evaluation deltas).\n",
        "          * **Reward Function**: Use your existing `EvaluationEngine.evaluate_position_from_perspective()` output as a granular reward signal at each step, in addition to terminal game results. For example:\n",
        "              * Win: +1000 points\n",
        "              * Draw: +500 points\n",
        "              * Loss: -1000 points\n",
        "              * Evaluation gain from previous move: +X points (where X is the eval difference)\n",
        "              * Evaluation loss from previous move: -Y points\n",
        "      * **`select_parents()`**: Chooses fitter individuals from the population (e.g., roulette wheel selection, tournament selection).\n",
        "      * **`crossover(parent1, parent2)`**: Combines parameters from two parent models to create offspring.\n",
        "      * **`mutate(model)`**: Randomly alters some parameters of an offspring based on `mutation_rate` (e.g., slightly adjust a `checkmate_bonus` or `pst_weight`).\n",
        "\n",
        "### 2.2.2 Integration and Training Workflow\n",
        "\n",
        "1.  **Define Genetic Parameters**: Explicitly list which parameters from your `config.yaml` rulesets (`default_evaluation`, `aggressive_evaluation`, etc.) will be subject to genetic evolution. These become the \"genes\" of your AI.\n",
        "2.  **Training Script (`train_genetic.py` - New File)**:\n",
        "      * Initialize `GeneticAlgorithm`.\n",
        "      * Load your `EvaluationEngine` (the current one) as the \"template\" for parameter structure.\n",
        "      * Implement the main genetic loop:\n",
        "          * `initialize_population()`\n",
        "          * For each generation:\n",
        "              * For each `model` in the population:\n",
        "                  * Play `N` games using this `model`'s parameters.\n",
        "                  * Use your `chess_game.py` (which now supports loading external AI configs) to play games.\n",
        "                  * After each game, calculate `fitness` based on results and `EvaluationEngine`'s per-move scores.\n",
        "              * `select_parents()`\n",
        "              * `crossover()` and `mutate()` to create the next generation.\n",
        "          * Track and log the best performing individual (its parameters and fitness) over generations.\n",
        "      * **Store Best Parameters**: Save the `config.yaml` snippet of the best-performing genetic AI.\n",
        "\n",
        "### 2.2.3 Integration into `chess_game.py`\n",
        "\n",
        "1.  **New AI Type**: Add `ai_type: genetic_ai_engine` to your `ai_types` list in `config.yaml`.\n",
        "2.  **Dynamic Config Loading**: In `ChessGame._initialize_ai_engines()`, when `engine == 'Viper'` and `ai_type == 'genetic_ai_engine'`, load a saved genetic parameter set (e.g., from a specific YAML file `best_genetic_config.yaml`) and apply it to the `EvaluationEngine` instance. This means `EvaluationEngine` might need a method to `load_ruleset(config_dict)`.\n",
        "\n",
        "## 2.3 Viper Reinforcement AI Engine\n",
        "\n",
        "This AI learns through self-play and trial-and-error, using your evaluation scores as immediate rewards.\n",
        "\n",
        "### 2.3.1 Core Concepts (Based on `reinforcement_dot_ai.py` concepts and `self_play.py`)\n",
        "\n",
        "  * **State Representation**: Convert `chess.Board` into a state that an RL agent can understand (e.g., the same tensor representation as for the NN AI).\n",
        "  * **Action Space**: The set of all legal moves from a given state. You'll need `move_to_index`/`index_to_move` from your NN AI part.\n",
        "  * **Reward Function**: This is where your existing evaluation logic is invaluable.\n",
        "      * **Immediate Rewards**: The change in `EvaluationEngine.evaluate_position_from_perspective()` score after making a move. A positive change is a reward, a negative change is a penalty.\n",
        "      * **Terminal Rewards**: Large positive reward for winning (checkmate), a large negative reward for losing (checkmated), and a neutral reward for draws.\n",
        "  * **RL Algorithm**:\n",
        "      * **Q-Learning / SARSA**: A tabular Q-learning approach (like in `reinforcement_dot_ai.py`) is feasible for very small board states or simplified chess (e.g., endgames with few pieces). For full chess, a neural network is required to approximate the Q-function (Deep Q-Networks - DQN).\n",
        "      * **Policy Gradients (REINFORCE, Actor-Critic)**: More suitable for complex games like chess, where the network learns a policy (probability distribution over moves) directly.\n",
        "  * **Self-Play Loop**: Your `self_play.py` provides the foundation.\n",
        "      * An agent plays against itself (or a copy of itself).\n",
        "      * At each step, the agent chooses a move (with some exploration - epsilon-greedy).\n",
        "      * The environment (the chess board) updates, and a reward is calculated based on the immediate evaluation change.\n",
        "      * The experience (state, action, reward, next\\_state) is stored.\n",
        "      * The agent updates its policy/Q-values based on these experiences.\n",
        "\n",
        "### 2.3.2 Integration and Training Workflow\n",
        "\n",
        "1.  **Define RL Environment (`chess_env.py` - New File)**:\n",
        "      * A class that wraps `chess.Board`.\n",
        "      * `reset()`: Resets the board to a starting position.\n",
        "      * `step(action)`: Takes a `chess.Move` (action), applies it, returns `(new_state, reward, done, info)`.\n",
        "          * `new_state`: Tensor representation of the new board.\n",
        "          * `reward`: Calculated using `EvaluationEngine.evaluate_position_from_perspective()` changes and terminal game results.\n",
        "          * `done`: Boolean indicating if the game is over.\n",
        "          * `info`: Additional data (e.g., legal moves, actual evaluation).\n",
        "      * `get_legal_actions()`: Returns a list of legal moves (or their indices).\n",
        "2.  **RL Agent (`rl_agent.py` - New File)**:\n",
        "      * Contains the `ChessAI` (NN) from your supervised learning part, but now trained with an RL objective.\n",
        "      * Implements `choose_move(state, legal_moves)` (with exploration).\n",
        "      * Implements `learn(experience)`: Updates the NN's weights based on experience.\n",
        "3.  **Training Script (`train_rl.py` - New File)**:\n",
        "      * Main loop for self-play episodes.\n",
        "      * Each episode, instantiate `chess_env` and two `rl_agent`s (or one agent playing against itself).\n",
        "      * Generate games, collect experiences, and train the agent(s).\n",
        "      * **Save/Load Model**: Save the trained `state_dict` of the RL agent's `ChessAI` model.\n",
        "\n",
        "### 2.3.3 Integration into `chess_game.py`\n",
        "\n",
        "1.  **New AI Type**: Add `ai_type: reinforcement_ai_engine` to `config.yaml`.\n",
        "2.  **Load RL Model**: In `ChessGame._initialize_ai_engines()`, load the saved RL model and `MoveEncoder`.\n",
        "3.  **Use RL Agent for `search`**: The `search` method for this AI type would involve feeding the current board state into the loaded RL agent's NN to predict a move.\n",
        "\n",
        "## 2.4 V7P3R Chess AI Engine (Old NN Approach)\n",
        "\n",
        "This refers to your original Neural Network AI that trains specifically off *your* PGNs to attempt to play like you. The core components are very similar to the \"Viper NN AI Engine (Supervised Learning)\" described in 2.1, but with a stronger emphasis on filtering and training on your own games.\n",
        "\n",
        "### 2.4.1 Key Components and Differences\n",
        "\n",
        "  * **`chess_core.py` (specifically `ChessDataset`)**: Ensure your `ChessDataset` can filter PGNs to include only games where \"v7p3r\" (or your specific username) was a player. This is explicitly handled in your `filter_v7p3r_games` function in `train.py`.\n",
        "  * **`train.py`**: This script would be very similar to the supervised learning train script, but the dataset preparation would specifically filter for your games.\n",
        "  * **Objective**: The primary objective is accurate move prediction based on *your* playstyle, not necessarily absolute chess strength (though it can correlate).\n",
        "\n",
        "### 2.4.2 Integration into `chess_game.py`\n",
        "\n",
        "This is largely identical to the \"Viper NN AI Engine\" integration (2.1.2), but you would set `ai_type: v7p3r_nn_ai_engine` and potentially use a different model file path in `config.yaml`.\n",
        "\n",
        "## 2.5 Viper Hybrid NN-Search Engine (Additional ML/Eval Strategy)\n",
        "\n",
        "This approach combines the strengths of traditional search algorithms (like your refined Minimax/Negamax with alpha-beta pruning) with the pattern recognition capabilities of a Neural Network. This is the foundation of modern top-tier chess engines (like Stockfish + NNUE).\n",
        "\n",
        "### 2.5.1 Core Idea\n",
        "\n",
        "Instead of solely using your hand-crafted evaluation function (`_calculate_score`) at the leaf nodes of your search tree, you would use a trained Neural Network for evaluation.\n",
        "\n",
        "### 2.5.2 Components and Integration\n",
        "\n",
        "1.  **Trained Neural Network**: You'll need a pre-trained `ChessAI` (or a similar lightweight NN architecture focused on evaluation) from your \"Viper NN AI Engine\" or a dedicated evaluation-focused NN. This NN's `value_output` will be its primary output.\n",
        "2.  **Modify `EvaluationEngine.evaluate_position()`**:\n",
        "      * In your `evaluation_engine.py`, find `evaluate_position()` or `_calculate_score()`.\n",
        "      * Introduce a conditional check: If the AI type is \"Viper Hybrid NN-Search\", and a NN model is loaded, use the NN's `value_output` for evaluation. Otherwise, fall back to your existing rule-based evaluation.\n",
        "    <!-- end list -->"
      ],
      "metadata": {
        "id": "iPOUP_e5MOII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In EvaluationEngine (or a new HybridEvaluationEngine class)\n",
        "    # Assume self.nn_model and self.device are initialized if using this AI type\n",
        "    def evaluate_position_hybrid(self, board: chess.Board):\n",
        "        if self.ai_type == 'viper_hybrid_nn_search' and hasattr(self, 'nn_model') and self.nn_model:\n",
        "            with torch.no_grad():\n",
        "                board_tensor = board_to_tensor(board).unsqueeze(0).to(self.device)\n",
        "                _, value_eval = self.nn_model(board_tensor) # Only interested in value_eval\n",
        "                return value_eval.item() # Return the numerical evaluation\n",
        "\n",
        "        # Fallback to traditional evaluation for other AI types or if NN not loaded\n",
        "        return self.evaluate_position(board) # Your existing rule-based evaluation"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "stc2CepIMOIJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  **Search Algorithm Integration**: Your `_deep_search`, `_minimax_search`, `_negamax_search`, and `_negascout` functions (in `evaluation_engine.py`) would then call `evaluate_position_hybrid` (or directly use `self.nn_model` if deeply integrated) at their leaf nodes (when `depth == 0` or game is over).\n",
        "\n",
        "### 2.5.3 Training the Hybrid NN\n",
        "\n",
        "  * The NN component of the hybrid engine would be trained using supervised learning, similar to the \"Viper NN AI Engine,\" but specifically focused on producing accurate positional evaluations.\n",
        "  * **Data Sources**: You can use:\n",
        "      * Your own PGNs (extract board states and their final game results, or use deep search evaluations as target values).\n",
        "      * Lichess datasets (millions of games with Stockfish evaluations often included).\n",
        "  * **Reward Function**: The network learns to predict an accurate evaluation score for a position.\n",
        "\n",
        "## 2.6 GUI and Lichess Bot Compatibility\n",
        "\n",
        "The current `chess_game.py` and `stockfish_handler.py` are designed with the UCI protocol in mind. This is the key to compatibility.\n",
        "\n",
        "  * **UCI Compatibility**: The UCI protocol is the standard interface for chess engines. Any AI you develop (Viper NN, Genetic, Reinforcement, Hybrid) needs to expose a UCI-like interface. Your existing `EvaluationEngine.search()` and `evaluate_position_from_perspective()` methods fulfill this role within your current `chess_game.py` structure.\n",
        "      * If you were to create a standalone UCI interface (like `uci_interface.py` from old projects), it would simply instantiate your chosen AI engine and handle UCI commands (`position`, `go`, `isready`, etc.) by calling the engine's `search` and `evaluate_position_from_perspective` methods.\n",
        "  * **Lichess Bot Deployment**: Your `lichess_bot.py` already uses `chess.engine` and `requests` to interact with the Lichess API.\n",
        "      * The `LichessBot` class takes an `engine` parameter during initialization. You would simply pass an instance of your new AI engine class (e.g., `LichessBot(token, engine=ViperNNAIEngine(board, color, config))`) instead of `EvaluationEngine` directly.\n",
        "      * As long as your new AI classes implement the `search()` and `evaluate_position_from_perspective()` methods (or adapt to what the `LichessBot` expects for move generation and evaluation), integration should be straightforward.\n",
        "  * **Nibbler and Other GUIs**: These GUIs expect a UCI-compatible executable.\n",
        "      * You would use `PyInstaller` (as hinted in `package_exe.py`) to create a standalone executable for a dedicated UCI interface script. This script would act as a wrapper, taking UCI commands from the GUI and passing them to your chosen AI engine internally.\n",
        "      * The `package_uci_engine` function in `package_exe.py` provides a good starting point for creating an executable for a UCI interface for your custom engines.\n",
        "\n",
        "These two comprehensive guides should provide you with the detailed steps and conceptual understanding needed to implement your distributed computing setup and advanced AI engines. Good luck with your exciting experiments\\!"
      ],
      "metadata": {
        "id": "r-dAr0pGMOIJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"md-recitation\">\n",
        "  Sources\n",
        "  <ol>\n",
        "  <li><a href=\"https://github.com/brunomgv/api-gcp-secret-manager\">https://github.com/brunomgv/api-gcp-secret-manager</a></li>\n",
        "  <li><a href=\"https://www.screenshotapi.net/blog/deploying-a-playwright-container-using-docker-in-aws\">https://www.screenshotapi.net/blog/deploying-a-playwright-container-using-docker-in-aws</a></li>\n",
        "  <li><a href=\"https://github.com/Wegatriespython/Chess-RL\">https://github.com/Wegatriespython/Chess-RL</a></li>\n",
        "  <li><a href=\"https://github.com/askvyas/Monocular_Depth_Estimation\">https://github.com/askvyas/Monocular_Depth_Estimation</a></li>\n",
        "  <li><a href=\"https://github.com/TTitcombe/DQN\">https://github.com/TTitcombe/DQN</a></li>\n",
        "  </ol>\n",
        "</div>"
      ],
      "metadata": {
        "id": "HVDxaC8xMOIJ"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}